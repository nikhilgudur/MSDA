{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "\n",
    "def k_means(data, centroids, max_iter=100):\n",
    "    for _ in range(max_iter):\n",
    "        clusters = [[] for _ in range(len(centroids))]\n",
    "        for x in data:\n",
    "            distances = [euclidean_distance(x, c) for c in centroids]\n",
    "            cluster_index = np.argmin(distances)\n",
    "            clusters[cluster_index].append(x)\n",
    "\n",
    "        prev_centroids = centroids.copy()\n",
    "        centroids = [np.mean(cluster, axis=0) for cluster in clusters]\n",
    "\n",
    "        if np.all(np.array(prev_centroids) == np.array(centroids)):\n",
    "            break\n",
    "\n",
    "    return clusters, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case (i):\n",
      "Clusters: [[array([2, 3]), array([5, 7]), array([11, 13]), array([17, 19])], [array([29, 31]), array([37, 41]), array([47, 53]), array([59, 61]), array([67, 71])]]\n",
      "Final Centroids: [array([ 8.75, 10.5 ]), array([47.8, 51.4])]\n",
      "\n",
      "Case (ii):\n",
      "Clusters: [[array([2, 3]), array([5, 7]), array([11, 13]), array([17, 19]), array([29, 31])], [array([37, 41]), array([47, 53]), array([59, 61]), array([67, 71])]]\n",
      "Final Centroids: [array([12.8, 14.6]), array([52.5, 56.5])]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[2, 3], [5, 7], [11, 13], [17, 19], [29, 31], [37, 41], [47, 53], [59, 61], [67, 71]])\n",
    "\n",
    "centroids_i = np.array([[2, 3], [5, 7]])\n",
    "clusters_i, final_centroids_i = k_means(data, centroids_i)\n",
    "\n",
    "print(\"Case (i):\")\n",
    "print(\"Clusters:\", clusters_i)\n",
    "print(\"Final Centroids:\", final_centroids_i)\n",
    "\n",
    "centroids_ii = np.array([[2, 3], [67, 71]])\n",
    "clusters_ii, final_centroids_ii = k_means(data, centroids_ii)\n",
    "\n",
    "print(\"\\nCase (ii):\")\n",
    "print(\"Clusters:\", clusters_ii)\n",
    "print(\"Final Centroids:\", final_centroids_ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convergence speed: For case (i), where the initial centroids are close to each other, this case will have a faster speed of convergence, which is accredited to the fact that the points are relatively distributed uniformly between the two clusters. In case (ii), where far centroids are chosen, it takes more runs of the algorithm with more iterations to converge. This is because most points here will be close to one centroid more than the other, and eventually, there is a need for the centroids to move largely so that there is better partitioning.\n",
    "   \n",
    "2. Outlier-sensitive: The second case reiterates that the k-means algorithm is sensitive to outliers. When one of the initial centroids is placed on an outlier (in this case, the last data point [67, 71]), then this algorithm may produce a cluster that contains only that outlier; therefore, the other majority of the data point is contained in the other cluster. This can provide a less meaningful cluster result. In case (i), where the initial centroids are not placed at the outliers, this algorithm becomes less sensitive to outliers and offers a more balanced clustering result.\n",
    "   \n",
    "3. Importance of domain knowledge: The comparison of these two cases brings out clearly the importance of using the right domain knowledge during the application of the k-means clustering algorithm. If the user is well-informed about either the data set structure or clustering outcome in advance, he or she can use this information for making a better choice of initial centroids. For example, if the user knows that there are two well-separated clusters within the dataset, then one could expect that initializing centroids near the centers of the two clusters would yield a better result in clustering. On the other hand, if the user randomly chooses initial centroids without an idea of the structure of the dataset, then clustering may come out with very poor or imbalanced results.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
