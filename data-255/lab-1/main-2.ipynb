{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                     \"mps\" if torch.backends.mps.is_available() else\n",
    "                     \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m anime_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Anime Dataset 2023/anime-dataset-2023.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m user_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Anime Dataset 2023/users-details-2023.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m user_scores \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Anime Dataset 2023/users-score-2023.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset shapes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnime data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manime_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/msda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/msda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/msda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/msda/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "anime_data = pd.read_csv('./Anime Dataset 2023/anime-dataset-2023.csv')\n",
    "user_data = pd.read_csv('./Anime Dataset 2023/users-details-2023.csv')\n",
    "user_scores = pd.read_csv('./Anime Dataset 2023/users-score-2023.csv')\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"Anime data: {anime_data.shape}\")\n",
    "print(f\"User data: {user_data.shape}\")\n",
    "print(f\"User scores: {user_scores.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "Processed data shape: (17870130, 21)\n"
     ]
    }
   ],
   "source": [
    "# Merge datasets\n",
    "print(\"Preprocessing data...\")\n",
    "data = pd.merge(user_scores, user_data, left_on='user_id', right_on='Mal ID')\n",
    "data = pd.merge(data, anime_data, on='anime_id')\n",
    "\n",
    "# Filter popular anime and active users\n",
    "popular_anime = data['anime_id'].value_counts()\n",
    "popular_anime_ids = popular_anime[popular_anime > 500].index\n",
    "data = data[data['anime_id'].isin(popular_anime_ids)]\n",
    "\n",
    "active_users = data['user_id'].value_counts()\n",
    "active_user_ids = active_users[active_users > 100].index\n",
    "data = data[data['user_id'].isin(active_user_ids)]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\n",
    "    'Gender', 'Birthday', 'Location', 'Username_x', 'Username_y',\n",
    "    'Mal ID', 'Anime Title', 'Name', 'English name', 'Other name',\n",
    "    'Synopsis', 'Aired', 'Premiered', 'Producers', 'Licensors',\n",
    "    'Studios', 'Source', 'Image URL', 'Joined', 'Status',\n",
    "    'Duration', 'Rank', 'Scored By'\n",
    "]\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Fill missing values\n",
    "num_cols = [\n",
    "    'Days Watched', 'Mean Score', 'Watching', 'Completed',\n",
    "    'On Hold', 'Dropped', 'Plan to Watch', 'Total Entries',\n",
    "    'Rewatched', 'Episodes Watched'\n",
    "]\n",
    "for col in num_cols:\n",
    "    data[col].fillna(data[col].mean(), inplace=True)\n",
    "\n",
    "print(f\"Processed data shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating genre features...\n",
      "Creating interaction features...\n",
      "Number of features: 32\n"
     ]
    }
   ],
   "source": [
    "# Create genre features\n",
    "print(\"Creating genre features...\")\n",
    "genres = anime_data['Genres'].str.split(',', expand=True).stack()\n",
    "genre_dummies = pd.get_dummies(genres, prefix='genre')\n",
    "genre_features = genre_dummies.groupby(level=0).sum()\n",
    "\n",
    "# Keep top genres\n",
    "top_genres = genre_features.sum().nlargest(15).index\n",
    "genre_features = genre_features[top_genres]\n",
    "\n",
    "# Create interaction features\n",
    "print(\"Creating interaction features...\")\n",
    "data['user_rating_diff'] = data['rating'] - data['Mean Score']\n",
    "data['popularity_score'] = np.log1p(data['Popularity']) * data['rating']\n",
    "data['completion_rate'] = data['Completed'] / (data['Total Entries'] + 1)\n",
    "data['watch_intensity'] = data['Episodes Watched'] / (data['Days Watched'] + 1)\n",
    "\n",
    "# Merge genre features\n",
    "data = pd.merge(data, genre_features, left_on='anime_id', right_index=True)\n",
    "\n",
    "# Define final feature list\n",
    "numeric_features = [\n",
    "    'Days Watched', 'Mean Score', 'Watching', 'Completed',\n",
    "    'On Hold', 'Dropped', 'Plan to Watch', 'Total Entries',\n",
    "    'Rewatched', 'Episodes Watched', 'Popularity', 'Favorites',\n",
    "    'Members', 'completion_rate', 'watch_intensity', 'user_rating_diff',\n",
    "    'popularity_score'\n",
    "] + genre_features.columns.tolist()\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "data[numeric_features] = scaler.fit_transform(data[numeric_features])\n",
    "\n",
    "print(f\"Number of features: {len(numeric_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ID mappings...\n",
      "Number of unique users: 75459\n",
      "Number of unique anime: 3361\n"
     ]
    }
   ],
   "source": [
    "# Create ID mappings\n",
    "print(\"Creating ID mappings...\")\n",
    "user_encoder = LabelEncoder()\n",
    "anime_encoder = LabelEncoder()\n",
    "\n",
    "data['user_id_mapped'] = user_encoder.fit_transform(data['user_id'])\n",
    "data['anime_id_mapped'] = anime_encoder.fit_transform(data['anime_id'])\n",
    "\n",
    "# Save encoders\n",
    "encoders = {\n",
    "    'user_encoder': user_encoder,\n",
    "    'anime_encoder': anime_encoder,\n",
    "    'scaler': scaler\n",
    "}\n",
    "\n",
    "with open('encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(encoders, f)\n",
    "\n",
    "print(\"Number of unique users:\", len(user_encoder.classes_))\n",
    "print(\"Number of unique anime:\", len(anime_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ID mappings...\n",
      "Number of unique users: 75459\n",
      "Number of unique anime: 3361\n"
     ]
    }
   ],
   "source": [
    "# Create ID mappings\n",
    "print(\"Creating ID mappings...\")\n",
    "user_encoder = LabelEncoder()\n",
    "anime_encoder = LabelEncoder()\n",
    "\n",
    "data['user_id_mapped'] = user_encoder.fit_transform(data['user_id'])\n",
    "data['anime_id_mapped'] = anime_encoder.fit_transform(data['anime_id'])\n",
    "\n",
    "# Save encoders\n",
    "encoders = {\n",
    "    'user_encoder': user_encoder,\n",
    "    'anime_encoder': anime_encoder,\n",
    "    'scaler': scaler\n",
    "}\n",
    "\n",
    "with open('encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(encoders, f)\n",
    "\n",
    "print(\"Number of unique users:\", len(user_encoder.classes_))\n",
    "print(\"Number of unique anime:\", len(anime_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepRecommender(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_features, embed_dim=64):\n",
    "        super(WideAndDeepRecommender, self).__init__()\n",
    "\n",
    "        # Wide Component\n",
    "        self.wide = nn.Sequential(\n",
    "            nn.Linear(n_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        # Deep Component\n",
    "        self.user_embedding = nn.Embedding(n_users, embed_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embed_dim)\n",
    "\n",
    "        self.deep = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, std=0.01)\n",
    "\n",
    "    def forward(self, user_ids, anime_ids, features):\n",
    "        # Wide Component\n",
    "        wide_out = self.wide(features)\n",
    "\n",
    "        # Deep Component\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        anime_emb = self.item_embedding(anime_ids)\n",
    "        deep_input = torch.cat([user_emb, anime_emb], dim=1)\n",
    "        deep_out = self.deep(deep_input)\n",
    "\n",
    "        # Combine outputs\n",
    "        combined_out = wide_out + deep_out\n",
    "        return torch.clamp(combined_out, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation completed.\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and targets\n",
    "X = data[numeric_features].values\n",
    "user_ids = data['user_id_mapped'].values\n",
    "anime_ids = data['anime_id_mapped'].values\n",
    "ratings = data['rating'].values\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.FloatTensor(X)\n",
    "user_ids = torch.LongTensor(user_ids)\n",
    "anime_ids = torch.LongTensor(anime_ids)\n",
    "ratings = torch.FloatTensor(ratings).view(-1, 1)\n",
    "\n",
    "# Create dataset\n",
    "dataset = TensorDataset(user_ids, anime_ids, X, ratings)\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, test_size]\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "print(\"Data preparation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    with tqdm(train_loader, desc=\"Training\") as pbar:\n",
    "        for user_id, anime_id, features, rating in pbar:\n",
    "            user_id = user_id.to(device)\n",
    "            anime_id = anime_id.to(device)\n",
    "            features = features.to(device)\n",
    "            rating = rating.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(user_id, anime_id, features)\n",
    "            loss = criterion(prediction, rating)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for user_id, anime_id, features, rating in test_loader:\n",
    "            user_id = user_id.to(device)\n",
    "            anime_id = anime_id.to(device)\n",
    "            features = features.to(device)\n",
    "            rating = rating.to(device)\n",
    "\n",
    "            prediction = model(user_id, anime_id, features)\n",
    "            loss = criterion(prediction, rating)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions.extend(prediction.cpu().numpy())\n",
    "            actuals.extend(rating.cpu().numpy())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "\n",
    "    rmse = np.sqrt(((predictions - actuals) ** 2).mean())\n",
    "    mae = np.abs(predictions - actuals).mean()\n",
    "    accuracy = np.mean(np.abs(predictions - actuals) <= 1.0)\n",
    "\n",
    "    return {\n",
    "        'test_loss': total_loss / len(test_loader),\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93785/93785 [30:22<00:00, 51.47it/s, loss=0.0002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0152\n",
      "Test Loss: 0.0570\n",
      "RMSE: 0.2387\n",
      "MAE: 0.1509\n",
      "Accuracy (±1): 99.55%\n",
      "Saved new best model\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "n_users = len(user_encoder.classes_)\n",
    "n_items = len(anime_encoder.classes_)\n",
    "n_features = len(numeric_features)\n",
    "\n",
    "model = WideAndDeepRecommender(\n",
    "    n_users=n_users,\n",
    "    n_items=n_items,\n",
    "    n_features=n_features,\n",
    "    embed_dim=64\n",
    ").to(device)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', patience=2, factor=0.5, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Test Loss: {metrics['test_loss']:.4f}\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"MAE: {metrics['mae']:.4f}\")\n",
    "    print(f\"Accuracy (±1): {metrics['accuracy']:.2%}\")\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(metrics['test_loss'])\n",
    "\n",
    "    # Save best model\n",
    "    if metrics['test_loss'] < best_loss:\n",
    "        best_loss = metrics['test_loss']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "            'encoders': encoders\n",
    "        }, 'best_model.pth')\n",
    "        print(\"Saved new best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     41\u001b[0m sample_user_id \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 42\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mget_recommendations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_user_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTop 10 Recommendations:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m display(recommendations)\n",
      "Cell \u001b[0;32mIn[16], line 20\u001b[0m, in \u001b[0;36mget_recommendations\u001b[0;34m(model, user_id, top_n)\u001b[0m\n\u001b[1;32m     18\u001b[0m user_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(user_ids)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m anime_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(all_anime_ids)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 20\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_matrix\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "def get_recommendations(model, user_id, top_n=10):\n",
    "    \"\"\"Generate anime recommendations for a user\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Get user features\n",
    "    user_data = data[data['user_id'] == user_id].iloc[0]\n",
    "    user_id_mapped = user_encoder.transform([user_id])[0]\n",
    "\n",
    "    # Create prediction matrix for all anime\n",
    "    all_anime_ids = anime_encoder.transform(anime_encoder.classes_)\n",
    "    user_ids = np.full_like(all_anime_ids, user_id_mapped)\n",
    "\n",
    "    # Prepare features\n",
    "    user_features = user_data[numeric_features].values\n",
    "    feature_matrix = np.tile(user_features, (len(all_anime_ids), 1))\n",
    "\n",
    "    # Convert to tensors\n",
    "    user_ids = torch.LongTensor(user_ids).to(device)\n",
    "    anime_ids = torch.LongTensor(all_anime_ids).to(device)\n",
    "    features = torch.FloatTensor(feature_matrix).to(device)\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        predictions = model(user_ids, anime_ids, features)\n",
    "\n",
    "    # Get top N recommendations\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    top_indices = predictions.flatten().argsort()[-top_n:][::-1]\n",
    "\n",
    "    # Get recommended anime details\n",
    "    recommended_anime_ids = anime_encoder.inverse_transform(all_anime_ids[top_indices])\n",
    "    recommendations = anime_data[anime_data['anime_id'].isin(recommended_anime_ids)][\n",
    "        ['anime_id', 'Anime Title', 'Type', 'Genres']\n",
    "    ].copy()\n",
    "\n",
    "    recommendations['predicted_rating'] = predictions[top_indices]\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "sample_user_id = data['user_id'].iloc[0]\n",
    "recommendations = get_recommendations(model, sample_user_id)\n",
    "print(\"\\nTop 10 Recommendations:\")\n",
    "display(recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
